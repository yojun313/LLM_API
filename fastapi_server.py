from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import datetime
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama import OllamaLLM

# uvicorn fastapi_server:app --host 0.0.0.0 --port 3333 --reload

app = FastAPI()

# ğŸ”¹ ëª¨ë¸ ìºì‹± (ì´ˆê¸°ì—ëŠ” None, ìš”ì²­ì´ ì˜¤ë©´ ìƒì„±)
ollama_cache = {
    "current_model": None,  # í˜„ì¬ í™œì„±í™”ëœ ëª¨ë¸ ì´ë¦„
    "llm_instance": None   # í˜„ì¬ í™œì„±í™”ëœ ëª¨ë¸ ê°ì²´
}

class RequestData(BaseModel):
    model_name: str
    question: str

def generator(model, text):
    """ ëª¨ë¸ì´ ìš”ì²­ë  ë•Œë§Œ ìƒì„±í•˜ê³  ìœ ì§€í•˜ë©°, ë‹¤ë¥¸ ëª¨ë¸ ìš”ì²­ ì‹œ êµì²´ """
    if ollama_cache["current_model"] != model:
        print(f"ğŸ”„ ëª¨ë¸ ë³€ê²½: {ollama_cache['current_model']} â†’ {model}")
        ollama_cache["llm_instance"] = OllamaLLM(model=model, streaming=True, num_threads=4)
        ollama_cache["current_model"] = model

    llm = ollama_cache["llm_instance"]
    prompt = PromptTemplate.from_template(template=text)
    chain = prompt | llm | StrOutputParser()

    result = chain.invoke({})
    return result

def save_to_file(model_name, question, answer, filename="C:/GitHub/llm_history.txt"):
    """ ëª¨ë¸ ì´ë¦„, ì§ˆë¬¸, ë‹µë³€ì„ íŒŒì¼ì— ì €ì¥ """
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(filename, "a", encoding="utf-8") as file:
        file.write(f"[ {timestamp} ] - {model_name}\n")
        file.write(f"Q. {question}\n")
        file.write(f"A. {answer}\n")
        file.write("=" * 50 + "\n\n")  # êµ¬ë¶„ì„  ì¶”ê°€

@app.post("/api/process")
async def generate_response(data: RequestData):
    """ ì²« ìš”ì²­ì´ ì˜¤ë©´ ëª¨ë¸ì„ ìƒì„±í•˜ê³  ìœ ì§€, ë‹¤ë¥¸ ëª¨ë¸ ìš”ì²­ì´ ì˜¤ë©´ êµì²´ """
    if not data.model_name or not data.question:
        raise HTTPException(status_code=400, detail="Both model_name and question are required")
    
    answer = generator(data.model_name, data.question)
    save_to_file(data.model_name, data.question, answer)
    return {"result": answer}

@app.get("/models")
async def get_models():
    """ í˜„ì¬ í™œì„±í™”ëœ ëª¨ë¸ì„ ë°˜í™˜ """
    return {
        "current_model": ollama_cache["current_model"]
    }
