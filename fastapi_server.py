from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import datetime
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama import OllamaLLM

app = FastAPI()

# ğŸ”¹ ì‚¬ìš©í•  LLM ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
llm_list = [
    'deepseek-r1:14b',
    'llama3.1-instruct-8b',
    'deepseek-r1:70b',
    'llama3.3',
    'gemma:7b',
    'gemma2:27b'
]

# ğŸ”¹ ëª¨ë¸ ìºì‹± (ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë“  ëª¨ë¸ì„ ë¯¸ë¦¬ ìƒì„±)
ollama_cache = {model: OllamaLLM(model=model, streaming=True, num_threads=4) for model in llm_list}

class RequestData(BaseModel):
    model_name: str
    question: str

def generator(model, text):
    """ ìš”ì²­ëœ ëª¨ë¸ì´ ë¯¸ë¦¬ ìºì‹±ë˜ì–´ ìˆë‹¤ë©´ ë°”ë¡œ ì‚¬ìš© """
    if model not in ollama_cache:
        return f"Error: Model '{model}' is not available."

    llm = ollama_cache[model]
    prompt = PromptTemplate.from_template(template=text)
    chain = prompt | llm | StrOutputParser()

    result = chain.invoke({})
    return result

def save_to_file(model_name, question, answer, filename="C:/GitHub/llm_history.txt"):
    """ ëª¨ë¸ ì´ë¦„, ì§ˆë¬¸, ë‹µë³€ì„ íŒŒì¼ì— ì €ì¥ """
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(filename, "a", encoding="utf-8") as file:
        file.write(f"[ {timestamp} ]\n")
        file.write(f"Question: {question}\n")
        file.write(f"{model_name}: {answer}\n")
        file.write("=" * 50 + "\n")  # êµ¬ë¶„ì„  ì¶”ê°€

@app.post("/api/process")
async def generate_response(data: RequestData):
    """ ìš”ì²­ëœ ëª¨ë¸ë¡œ ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ ë°˜í™˜ (Node.jsì™€ ë™ì¼í•œ ìš”ì²­ ë°©ì‹) """
    if not data.model_name or not data.question:
        raise HTTPException(status_code=400, detail="Both model_name and question are required")

    answer = generator(data.model_name, data.question)
    save_to_file(data.model_name, data.question, answer)
    return {"result": answer}

@app.get("/models")
async def get_models():
    """ í˜„ì¬ ìºì‹±ëœ ëª¨ë¸ ëª©ë¡ì„ ë°˜í™˜ """
    return {"available_models": list(ollama_cache.keys())}
